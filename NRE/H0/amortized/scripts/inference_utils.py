# --- Importations ---------------------------------------------------------

# Python libraries
import h5py
import os
import numpy as np
import matplotlib.pyplot as plt

from astropy.cosmology import FlatLambdaCDM
from astropy.constants import c
c = c.to('Mpc/d')  # Speed of light

# PyTorch libraries
import torch
from torchquad import Simpson, set_up_backend
if torch.cuda.is_available():
    set_up_backend("cuda:0")

from simulator import mtx_rot, norm, pol2cart, cart2pol

torch.manual_seed(0)
np.random.seed(0)


# --- Functions for training -----------------------------------------------

def gaussian_noise(x, sig_dt=.3, sig_pot=.003):
    """
    Adds noise to time delays
    Inputs
        x : (tensor)[batch_size x 4 x 2] Time delays and Fermat potentials
        sig_dt : (float) noise standard deviation on time delays
        sig_pot : (float) noise standard deviation on potentials
    Outputs
        noisy_data : (tensor)[batch_size x 4 x 2] noisy time delays + true Fermat potential
    """
    mask_zero = torch.where(x[:, :, 0] == 0, 0, 1)
    mask_pad = torch.where(x[:, :, 0] == -1, 0, 1)
    noise_dt = sig_dt * torch.randn((x.size(0), x.size(1)))
    noise_pot = sig_pot * torch.randn((x.size(0), x.size(1)))
    noisy_data = x.clone()
    noisy_data[:, :, 0] += noise_dt * mask_zero * mask_pad
    noisy_data[:, :, 1] += noise_pot * mask_zero * mask_pad

    return noisy_data


def gaussian(x, mu, sigma):
    """
        Computes on multivariate gaussian with a diagonal covariance matrix
        Inputs
            x : (array) variable
            mu : (array) mean
            sigma : (float) standard deviation
        Outputs
            gauss : (float) Gaussian's value at x
        """
    gauss = torch.exp(torch.sum(-(x - mu) ** 2) / 2 / sigma ** 2) / np.sqrt(2 * np.pi * sigma ** 2)
    return gauss


def get_time_delays(fermat, zd, Dd, Ds, Dds):
    """
    Function to integrate to estimate the posterior distribution
    Inputs
        fermat : (float) Potential value
        H0 : (float) H0 value
        zs : (float) Source's redshift
        zd : (float) Deflector's redshift
    Outputs
        time_delays : (float) Time delay generated by fermat
    """
    time_delays = (1 + zd) * Dd.value * Ds.value / Dds.value / c.value * fermat
    time_delays *= (2 * np.pi / 360 / 3600) ** 2  # Conversion to days
    if np.count_nonzero(fermat + 1) == 2:
        time_delays[2:] = -np.ones(2)

    return time_delays


def get_Fermat_potentials(x0_lens, y0_lens, theta_E, ellip, phi, gamma_ext, phi_ext, xim_fov, yim_fov, x0_AGN=0., y0_AGN=0.):
    """Computes the position and the magnification of AGN images
    Also outputs the Fermat potential at these positions"""

    # Parameters
    f = ellip
    f_prime = np.sqrt(1 - f ** 2)

    # Image positions in the lens coordinate system
    xim, yim = np.tensordot(mtx_rot(-phi), np.array([xim_fov - x0_lens, yim_fov - y0_lens]), axes=1)
    r_im, phi_im = cart2pol(xim, yim)
    # AGN positions in the lens coordinate system
    xsrc, ysrc = np.tensordot(mtx_rot(-phi), np.array([x0_AGN - x0_lens, y0_AGN - y0_lens]), axes=1)
    # Coordinate translation for the external shear
    xtrans, ytrans = np.tensordot(mtx_rot(-phi), np.array([x0_lens, y0_lens]), axes=1)

    # External shear in the lens coordinate system
    gamma1_fov, gamma2_fov = pol2cart(gamma_ext, phi_ext)
    gamma1, gamma2 = np.tensordot(mtx_rot(-2 * phi), np.array([gamma1_fov, gamma2_fov]), axes=1)

    # X deflection angle (eq. 27a Kormann et al. 1994)
    def alphax(varphi):
        return theta_E * np.sqrt(f) / f_prime * np.arcsin(f_prime * np.sin(varphi))

    # Y deflection angle (eq. 27a Kormann et al. 1994)
    def alphay(varphi):
        return theta_E * np.sqrt(f) / f_prime * np.arcsinh(f_prime / f * np.cos(varphi))

    # Lens potential deviation from a SIS (Meneghetti, 19_2018, psi_tilde)
    def psi_tilde(varphi):
        return np.sin(varphi) * alphax(varphi) + np.cos(varphi) * alphay(varphi)

    # Lens potential (eq. 26a Kormann et al. 1994)
    def lens_pot(varphi):
        return psi_tilde(varphi) * radial(varphi)

    # Shear potential (eq. 3.80 Meneghetti)
    def shear_pot(x, y):
        return gamma1 / 2 * (x ** 2 - y ** 2) + gamma2 * x * y

    # Radial componant (eq. 34 Kormann et al. 1994)
    def radial(varphi):
        usual_rad = ysrc * np.cos(varphi) + xsrc * np.sin(varphi) + psi_tilde(varphi)
        translation = gamma1 * (xtrans * np.sin(varphi) - ytrans * np.cos(varphi)) + gamma2 * (
                    xtrans * np.cos(varphi) + ytrans * np.sin(varphi))
        shear_term = 1 + gamma1 * (np.cos(varphi) ** 2 - np.sin(varphi) ** 2) - 2 * gamma2 * np.cos(
            varphi) * np.sin(varphi)
        return (usual_rad + translation) / shear_term

    # Fermat potential
    geo_term = norm(xim - xsrc, yim - ysrc) ** 2 / 2
    lens_term = lens_pot(phi_im)
    shear_term = shear_pot(xim + xtrans, yim + ytrans)
    fermat_pot = geo_term - lens_term - shear_term

    fermat_pot = fermat_pot - np.min(fermat_pot)
    if np.count_nonzero(xim_fov + 1.) == 2:
        fermat_pot[2:] = -np.ones(2)

    return fermat_pot


def analytical_posterior(time_delays, fermat_pot, H0, xim_fov, yim_fov, zs, zd, sig_dt=.3, sig_pot=.003):
    """
    Computes the analytical posterior
    Inputs
        time_delays : (array) [nsamp x 3] Measured time delays
        fermat_pot : (array) [nsamp x 3] Modeled Fermat potentials
        H0 : (array) [npts_post] Grid of H0 values
        zs : (array) Source's redshift
        zd : (array) Deflector's redshift
        sig_dt : (float) Time delays standard deviation
        sig_pot : (float) Fermat potentils standard deviation
    Outputs
        post : (array)[nsamp x npts] posterior
    """

    nsamp = time_delays.shape[0]
    npts = H0.shape[0]
    post = torch.ones((nsamp, npts)).to(device)
    simp = Simpson()

    for i in range(nsamp):
        dt_obs = torch.from_numpy(time_delays[i]).to(device)
        pot_obs = torch.from_numpy(fermat_pot[i]).to(device)
        xim = torch.from_numpy(xim_fov[i]).to(device)
        yim = torch.from_numpy(yim_fov[i]).to(device)

        limit_v_disp = ([225., 275.]* u.km / u.s).to('Mpc/d') # x[0]
        limit_x0_lens = [-.3, .3] # x[1]
        limit_y0_lens = [-.3, .3] # x[2]
        limit_ellip = [.3, .99] # x[3]
        limit_phi = [-np.pi/2, np.pi/2] # x[4]
        limit_gamma_ext = [0., .05] # x[5]
        limit_phi_ext = [-np.pi/2, np.pi/2] # x[6]

        for j in range(npts):

            def integrand(x):
                """
                Function to integrate to estimate the posterior distribution
                Inputs
                    x : sample
                Outputs
                    I : (float) Evaluation of the integrand
                """
                cosmo_model = FlatLambdaCDM(H0=H0[j], Om0=.3)
                Ds = cosmo_model.angular_diameter_distance(zs[i])
                Dd = cosmo_model.angular_diameter_distance(zd[i])
                Dds = cosmo_model.angular_diameter_distance_z1z2(zd[i], zs[i])
                theta_E = 4 * np.pi * (x[0] / c) ** 2 * Dds / Ds * 180 * 3600 / np.pi
                pot = get_Fermat_potentials(x[1], x[2], theta_E, x[3], x[4], x[5], x[6], xim, yim)
                dt = get_time_delays(pot, zd[i], Dd, Ds, Dds)
                I = gaussian(dt_obs, dt, sig_dt) * gaussian(pot_obs, pot, sig_pot)

                return I

            I = simp.integrate(integrand, dim=7, N=10000,
                                integration_domain=[limit_v_disp, limit_x0_lens,
                                                    limit_y0_lens, limit_ellip,
                                                    limit_phi, limit_gamma_ext,
                                                    limit_phi_ext])
            post[i, j] = I

    return post.detach().cpu().numpy()


def log_trick(logp):
    """
    Compute the log-trick for numerical stability
    Inputs :
      log_p : log of a quantity to sum in a log
    Outputs :
      trick : stabilization term
    """
    max = np.amax(logp)
    ind_max = np.argmax(logp)
    logp = np.delete(logp, ind_max)
    trick = max + np.log(1 + np.sum(np.exp(logp - max)))

    return trick


def r_estimator(model, x1, x2):
    """
    Likelihood ratio estimator
    Inputs
        x1 : (tensor)[nexamp x 1 x npix x npix] time delays mask
        x2 : (tensor)[nexamp x 1] Hubble constant
    Outputs
        lr : (array)[nexamp] likelihood ratio
    """
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

    x1 = x1.to(device, non_blocking=True).float()
    x2 = x2.to(device, non_blocking=True).float()

    model = model.to(device, non_blocking=True)
    model.eval()

    with torch.no_grad():
        output = model(x1, x2)

    sm = torch.nn.Softmax(dim=1)
    prob = sm(output)
    s = prob[:, 1].detach().cpu().numpy()
    lr = s / (1 - s)

    return lr


def normalization(x, y):
    """
        Normalizes distributions
        Inputs
            x : (array) H0 values
            y : (array) posterior probabilities
        Outputs
            y : (array) Norm
        """
    norm = np.trapz(y, x, axis=1)
    y /= norm[:, None]

    return y


def inference(file_keys, file_data, file_model, path_out, nrow=5, ncol=4, npts=1000, batch_size=100):
    """
    Computes the NRE posterior, the analytical posterior and performs the coverage diagnostic
    Inputs
        file_keys : (str) name of the file containing the keys of the test set
        file_data : (str) name of the file containing data
        file_model : (str) name of the file containing the model
        path_out : (str) directory where to save the output
    Outputs
        None
    """
    if torch.cuda.is_available():
        model = torch.load(file_model)
    else:
        model = torch.load(file_model, map_location="cpu")

    # import keys
    keys = h5py.File(file_keys, 'r')
    test_keys = keys["test"][:]
    keys.close()

    # import data
    dataset = h5py.File(file_data, 'r')
    truths = dataset["Hubble_cst"][test_keys]
    lower_bound = np.floor(np.min(truths))
    higher_bound = np.ceil(np.max(truths))

    # remove out of bounds data
    idx_up = np.where(truths > 75.)[0]
    idx_down = np.where(truths < 65.)[0]
    idx_out = np.concatenate((idx_up, idx_down))
    truths = np.delete(truths, idx_out, axis=0)
    truths = truths.flatten()
    test_keys = np.delete(test_keys, idx_out, axis=0)
    dt = dataset["time_delays"][test_keys]
    pot = dataset["Fermat_potential"][test_keys]
    z = dataset["redshifts"][test_keys]
    im_pos = dataset["positions"][test_keys]
    dataset.close()

    # reshape data
    samples = np.concatenate((dt[:, :, None], pot[:, :, None]), axis=2)
    samples = samples[:4000]
    nsamp = samples.shape[0]

    # observations
    noisy_data = gaussian_noise(torch.from_numpy(samples))
    examples = noisy_data[noisy_data != 0]
    examples = examples.reshape(nsamp, 3, 2)
    examples_repeated = torch.repeat_interleave(examples, npts, dim=0)

    # Global NRE posterior
    support = np.linspace(lower_bound + 1, higher_bound - 1, npts).reshape(npts, 1)
    support_tile = np.tile(support, (nsamp, 1))

    dataset_test = torch.utils.data.TensorDataset(examples_repeated, torch.from_numpy(support_tile))
    dataloader = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)
    ratios = []
    for x1, x2 in dataloader:
        preds = r_estimator(model, x1, x2)
        ratios.extend(preds)

    ratios = np.asarray(ratios).reshape(nsamp, npts)
    support_tile = support_tile.reshape(nsamp, npts)
    ratios = normalization(support_tile, ratios)

    # predictions
    arg_pred = np.argmax(ratios, axis=1)
    pred = support_tile[np.arange(nsamp), arg_pred]

    # analytical posterior
    support = support.flatten()
    analytic = analytical_posterior(noisy_data[:, :, 0].numpy(), noisy_data[:, :, 1].numpy(), support, im_pos[:,0],
                                    im_pos[:, 1], z[:, 1], z[:, 0])
    analytic = normalization(support, analytic)

    it = 0
    fig, axes = plt.subplots(ncols=ncol, nrows=nrow, sharex=True, sharey=False, figsize=(3 * ncol, 3 * nrow))
    for i in range(nrow):
        for j in range(ncol):
            axes[i, j].plot(support, analytic[it], '--g', label='{:.2f}'.format(support[np.argmax(analytic[it])]))
            axes[i, j].plot(support, ratios[it], '-b', label='{:.2f}'.format(pred[it]))
            min_post = np.minimum(np.min(ratios[it]), np.min(analytic[it]))
            max_post = np.maximum(np.max(ratios[it]), np.max(analytic[it]))
            axes[i, j].vlines(truths[it], min_post, max_post, colors='r', linestyles='dotted',
                              label='{:.2f}'.format(truths[it]))
            axes[i, j].legend(frameon=False, borderpad=.2, handlelength=.6, fontsize=9, handletextpad=.4)
            if np.count_nonzero(samples[it, :, 0] + 1) == 3:
                axes[i, j].set_title("Quad")
            if np.count_nonzero(samples[it, :, 0] + 1) == 1:
                axes[i, j].set_title("Double")
            # axes[i, j].yaxis.set_major_formatter(FormatStrFormatter('%0.1f'))
            if i == int(nrow - 1):
                axes[i, j].set_xlabel(r"H$_0$ (km Mpc$^{-1}$ s$^{-1}$)")
            if j == 0:
                axes[i, j].set_ylabel(r"$p(H_0$ | $\Delta_t, \Delta_\phi)$")
            it += 1

    # saving
    plt.rcParams['axes.facecolor'] = 'white'
    plt.rcParams['savefig.facecolor'] = 'white'
    plt.savefig(path_out + '/posteriors.png', bbox_inches='tight')

    # file to save
    if os.path.isfile(path_out + "/posteriors.hdf5"):
        os.remove(path_out + "/posteriors.hdf5")
    post_file = h5py.File(path_out + "/posteriors.hdf5", 'a')

    NRE = post_file.create_group("NRE_global")
    H0 = NRE.create_dataset("Hubble_cst", (npts,), dtype='f')
    post = NRE.create_dataset("posterior", (nsamp, npts), dtype='f')
    post_anl = NRE.create_dataset("analytical", (nsamp, npts), dtype='f')

    truth_set = post_file.create_dataset("truth", (nsamp,), dtype='f')

    H0[:] = support
    post[:, :] = ratios
    post_anl[:, :] = analytic
    truth_set[:] = truths[:nsamp]

    post_file.close()

    # Highest probability density region
    credibility = np.zeros((nsamp,))
    for i in range(nsamp):

        idx_truth = np.where(abs(support - truths[i]) == np.min(abs(support - truths[i])))[0]
        probs = ratios[i]
        prob_truth = probs[idx_truth]

        prob_false = probs.copy()
        prob_false[idx_truth] = 0.
        idx_equal_prob = np.where(abs(prob_false - prob_truth) == np.min(abs(prob_false - prob_truth)))[0]

        if len(idx_equal_prob) != 1:
            if idx_equal_prob[0] == idx_truth - 1 and idx_equal_prob[1] == idx_truth + 1:
                credibility[i] = 0
            continue

        start = np.minimum(idx_truth, idx_equal_prob)
        end = np.maximum(idx_truth, idx_equal_prob)
        idx_HPD = np.arange(start, end)

        credibility[i] = np.trapz(probs[idx_HPD], support[idx_HPD])

    # Coverage diagnostic
    bins = np.linspace(0., 1., 100)
    emperical_coverage = np.zeros_like(bins)
    for i in range(len(bins)):
        emperical_coverage[i] = np.mean(np.where(credibility <= bins[i], 1, 0))

    plt.style.use(['dark_background'])
    plt.figure()
    plt.plot(bins, bins, '--', color='white')
    plt.plot(bins, emperical_coverage, '-', color='lime')
    plt.xlabel("Credibility")
    plt.ylabel("Emperical coverage")
    plt.text(0., .9, "Underconfident", fontsize='large')
    plt.text(.65, .05, "Overconfident", fontsize='large')
    # plt.rcParams['axes.facecolor'] = 'white'
    # plt.rcParams['savefig.facecolor'] = 'white'
    plt.savefig(path_out + '/coverage.png', bbox_inches='tight')


def joint_inference(file_data, file_model, path_out, npts=1000, lower_bound=60., upper_bound=80.):
    """
        Performs the joint inference on a population of lenses
        Inputs
            file_data : (str) name of the file containing data
            file_model : (str) name of the file containing the model
            path_out : (str) directory where to save the output
        Outputs
            None
    """
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    if device.type == "cpu":
        model = torch.load(file_model, map_location='cpu')
    else:
        model = torch.load(file_model)

    # import data
    dataset = h5py.File(file_data, 'r')
    H0 = dataset["Hubble_cst"][:]
    dt = dataset["time_delays"][:]
    pot = dataset["Fermat_potential"][:]
    dataset.close()
    samples = np.concatenate((dt[:, :, None], pot[:, :, None]), axis=2)
    samples = samples[:1000]
    nsamp = samples.shape[0]
    samples = samples[samples != 0]
    samples = samples.reshape(nsamp, 3, 2)
    true = float(np.unique(H0))

    # observations
    x = gaussian_noise(torch.from_numpy(samples))
    data = torch.repeat_interleave(x, npts, dim=0)

    # Global NRE posterior
    prior = np.linspace(lower_bound, upper_bound, npts).reshape(npts, 1)
    prior_tile = np.tile(prior, (nsamp, 1))
    prior = prior.flatten()
    post = r_estimator(model, data, torch.from_numpy(prior_tile))

    post = post.reshape(nsamp, npts)
    post = normalization(prior_tile.reshape(nsamp, npts), post)

    joint1 = np.sum(np.log(post[:10]), axis=0)
    joint1 -= log_trick(joint1)
    joint2 = np.sum(np.log(post[:100]), axis=0)
    joint2 -= log_trick(joint2)
    joint3 = np.sum(np.log(post[:1000]), axis=0)
    joint3 -= log_trick(joint3)
    joints = np.concatenate((joint1[:, None], joint2[:, None], joint3[:, None]), axis=1)

    plt.figure()
    plt.plot(prior, np.exp(joint1), '-b', label='10 lenses')
    plt.plot(prior, np.exp(joint2), '--r', label='100 lenses')
    plt.plot(prior, np.exp(joint3), '-.g', label='1000 lenses')
    min_post = np.min(np.exp(joints))
    max_post = np.max(np.exp(joints))
    plt.vlines(true, min_post, max_post, colors='k', linestyles='dotted', label="truth")
    plt.legend()
    plt.rcParams['axes.facecolor'] = 'white'
    plt.rcParams['savefig.facecolor'] = 'white'
    plt.savefig(path_out + '/inference.png', bbox_inches='tight')